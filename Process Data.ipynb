{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "np = pd.np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('EncCase19_test', delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align data files\n",
    "\n",
    "The observations file and stress files have a divergence in the lat/lon numbers do to truncation/rounding of the floating point values.  So we take the lat/lon values from the observations file and replace the lat/lon values in the stress file with the observation numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.read_csv('alldata.csv')\n",
    "obs = obs.append(obs.tail(1), ignore_index=True)\n",
    "repeated = pd.concat([obs] * 360, ignore_index=True)\n",
    "df[1] = repeated['lat']\n",
    "df[3] = repeated['lon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the stress data\n",
    "\n",
    "The imported data has repeated column labels treated as columns themselves.  So here we remove the label columns and transpose the first row of their values to become column headings on the remaining dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df.columns[1::2]]\n",
    "headings = df[df.columns[::2]].head(1)\n",
    "headings = headings.apply(lambda x: x.str.replace('=', ''))\n",
    "data.columns = headings.iloc[[0]].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop_duplicates().copy()\n",
    "\n",
    "# Clean up and allow the GC to handle datasets we no longer need\n",
    "data = None\n",
    "repeated = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert azimuth from clockwise to counter-clockwise and calc max stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['zetaCW'] = (360. - df['Degrees(zeta)'])\n",
    "df['maxStress'] = df[['sigThetaKPA', 'sigPhiKPA']].max(axis=1)\n",
    "\n",
    "df.loc[df.maxStress == df.sigPhiKPA, 'crackDir'] = df['zetaCW'] % 180\n",
    "df.loc[df.maxStress == df.sigThetaKPA, 'crackDir'] = ((df['zetaCW'] + 90) % 180) % 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in subset observation file and constrain data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.read_csv('observations.csv', sep='\\s*,\\s*', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(df, obs, left_on=['lonDeg', 'latDeg'], right_on=['W Lon', 'Gr Lat'], how='inner')\n",
    "merged.sort_values(['lonDeg', 'latDeg', 'Degrees(meanMotion)'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate intermediate values and PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['diff'] = merged['maxStress'] - merged.shift()['maxStress']\n",
    "merged.loc[merged['Degrees(meanMotion)'] == 0, \n",
    "           'diff'] = merged['maxStress'] - merged.shift(periods=-359)['maxStress']\n",
    "merged['pdf'] = merged['maxStress']\n",
    "merged.loc[merged['diff'] < 0, 'pdf'] = 0\n",
    "merged.loc[merged['maxStress'] < 0, 'pdf'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and calculating area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(group):\n",
    "    return (abs(np.trapz(group['pdf'], group['crackDir'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged.groupby(['latDeg', 'lonDeg'])\n",
    "\n",
    "areas = pd.DataFrame(grouped.apply(process_group))\n",
    "areas.columns = ['area']\n",
    "\n",
    "merged = pd.merge(merged, areas, on=['latDeg', 'lonDeg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['probability'] = merged['pdf'] / merged['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional - Testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_norm_group(group):\n",
    "    return (abs(np.trapz(group['probability'], group['crackDir'])))\n",
    "\n",
    "grouped = merged.groupby(['latDeg', 'lonDeg'])\n",
    "test = pd.DataFrame(grouped.apply(process_norm_group))\n",
    "\n",
    "test.columns = ['val']\n",
    "test.sort_values('val', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the interpolated probability for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_probability(group):\n",
    "    sorted_group = group.sort_values('crackDir')\n",
    "    azimuth = group['Degrees(azimuth)'].max()\n",
    "    return np.interp(azimuth, xp=sorted_group['crackDir'], fp=sorted_group['probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged.groupby(['latDeg', 'lonDeg', 'Degrees(azimuth)'])\n",
    "\n",
    "lat = []\n",
    "lon = []\n",
    "azimuth = []\n",
    "probability = []\n",
    "\n",
    "for name, group in grouped:\n",
    "    sorted_group = group.sort_values('crackDir')\n",
    "    lat.append(name[0])\n",
    "    lon.append(name[1])\n",
    "    azimuth.append(name[2])\n",
    "    probability.append(np.interp(name[2], xp=sorted_group['crackDir'], fp=sorted_group['probability']))\n",
    "    \n",
    "result = pd.DataFrame({ 'lat': lat, \n",
    "                      'lon': lon,\n",
    "                      'azimuth': azimuth,\n",
    "                      'probability': probability})\n",
    "\n",
    "display(result)\n",
    "\n",
    "# The following code also accomplishes the same result as the loop above,\n",
    "# but may be more challenging to consume due to how the columns are constructed\n",
    "\n",
    "# result = pd.DataFrame(grouped.apply(find_probability))\n",
    "# result.columns = ['prob']\n",
    "# # result.loc[-68.837121]\n",
    "# # result.sort_values('prob', ascending=False)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
